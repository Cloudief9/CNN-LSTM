import numpy as np
import pandas as pd
import h5py
from sklearn.model_selection import train_test_split
import keras
import keras.backend as K
import tensorflow.keras as tk
from keras.layers import LeakyReLU
from time import time
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

dataset_file = h5py.File("D:/data/Data Signals/archive/GOLD_XYZ_OSC.0001_1024.hdf5","r")

base_modulation_classes = ['OOK', '4ASK', '8ASK', 'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK',
                           '16APSK', '32APSK', '64APSK', '128APSK', '16QAM', '32QAM', '64QAM',
                           '128QAM', '256QAM', 'AM-SSB-WC', 'AM-SSB-SC', 'AM-DSB-WC', 'AM-DSB-SC',
                           'FM', 'GMSK', 'OQPSK']

selected_modulation_classes = ['4ASK', 'BPSK', 'QPSK', '16PSK', '16QAM', 'FM', 'AM-DSB-WC', '32APSK']

selected_classes_id = [base_modulation_classes.index(cls) for cls in selected_modulation_classes]

N_SNR = 4 # from 30 SNR to 22 SNR

X_data = None
y_data = None

for id in selected_classes_id:
    X_slice = dataset_file['X'][(106496*(id+1) - 4096*N_SNR):106496*(id+1)]
    y_slice = dataset_file['Y'][(106496*(id+1) - 4096*N_SNR):106496*(id+1)]
    
    if X_data is not None:
        X_data = np.concatenate([X_data, X_slice], axis=0)
        y_data = np.concatenate([y_data, y_slice], axis=0)
    else:
        X_data = X_slice
        y_data = y_slice

X_data = X_data.reshape(len(X_data), 32, 32, 2)

y_data_df = pd.DataFrame(y_data)
for column in y_data_df.columns:
    if sum(y_data_df[column]) == 0:
        y_data_df = y_data_df.drop(columns=[column])

y_data_df.columns = selected_modulation_classes
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data_df, test_size=0.2)


def create_model():
    learning_rate = 0.0001
    i_input = keras.layers.Input(shape=(32,32,1))
    q_input = keras.layers.Input(shape=(32,32,1))

    cnn_q_1 = tk.layers.Conv2D(64, 3, activation=LeakyReLU(alpha=0.1))(q_input)
    cnn_q_1_2 = tk.layers.Conv2D(64, 3, activation=LeakyReLU(alpha=0.1))(cnn_q_1)
    pool_q_1 = tk.layers.MaxPool2D(pool_size=3, strides=2, padding='valid')(cnn_q_1_2)
    cnn_q_2 = tk.layers.Conv2D(128, 3, activation=LeakyReLU(alpha=0.1))(pool_q_1)
    cnn_q_2_2 = tk.layers.Conv2D(128, 3, activation=LeakyReLU(alpha=0.1))(cnn_q_2)
    pool_q_2 = tk.layers.MaxPool2D(pool_size=3, strides=2, padding='valid')(cnn_q_2_2)
    flatten_q = tk.layers.Flatten()(pool_q_2)
    
    cnn_i_1 = tk.layers.Conv2D(64, 3, activation=LeakyReLU(alpha=0.1))(i_input)
    cnn_i_1_2 = tk.layers.Conv2D(64, 3, activation=LeakyReLU(alpha=0.1))(cnn_i_1)
    pool_i_1 = tk.layers.MaxPool2D(pool_size=3, strides=2, padding='valid')(cnn_i_1_2)
    cnn_i_2 = tk.layers.Conv2D(128, 3, activation=LeakyReLU(alpha=0.1))(pool_i_1)
    cnn_i_2_2 = tk.layers.Conv2D(128, 3, activation=LeakyReLU(alpha=0.1))(cnn_i_2)
    pool_i_2 = tk.layers.MaxPool2D(pool_size=3, strides=2, padding='valid')(cnn_i_2_2)
    flatten_i = tk.layers.Flatten()(pool_i_2)
    
    concat = keras.layers.concatenate([flatten_q, flatten_i])
    
    reshape = keras.layers.Reshape((1, -1))(concat)
    
    lstm = tk.layers.LSTM(256, return_sequences=False, dropout=0.5, recurrent_dropout=0.5)(reshape)
    
    dense1 = keras.layers.Dense(1024, activation=LeakyReLU(alpha=0.1))(lstm)
    dropout1 = tk.layers.Dropout(0.5)(dense1)
    
    dense2 = keras.layers.Dense(1024, activation=LeakyReLU(alpha=0.1))(dropout1)
    dropout2 = tk.layers.Dropout(0.5)(dense2)
    
    dense3 = keras.layers.Dense(1024, activation=LeakyReLU(alpha=0.1))(dropout2)
    dropout3 = tk.layers.Dropout(0.5)(dense3)
    
    dense4 = keras.layers.Dense(256, activation=LeakyReLU(alpha=0.1))(dropout3)
    dropout4 = tk.layers.Dropout(0.5)(dense4)
    
    dense5 = keras.layers.Dense(32, activation=LeakyReLU(alpha=0.1))(dropout4)
    outputs = keras.layers.Dense(len(selected_classes_id), activation='softmax')(dense5)
    
    model = keras.Model(inputs=[i_input, q_input], outputs=outputs)
    model.compile(
        loss='categorical_crossentropy', 
        optimizer=tk.optimizers.Adam(learning_rate=learning_rate),
        metrics=['accuracy']
    )
    return model


model = create_model()
batch_size = 64
epochs = 25

path_checkpoint = "model_checkpoint.weights.h5"
es_callback = tk.callbacks.EarlyStopping(monitor="accuracy", min_delta=0, patience=10)

modelckpt_callback = tk.callbacks.ModelCheckpoint(
    monitor="accuracy",
    filepath=path_checkpoint,
    verbose=1,
    save_weights_only=True,
    save_best_only=True,
)

history = model.fit(
    x=[X_train[:,:,:,0], X_train[:,:,:,1]],
    y=y_train,
    batch_size=batch_size,
    epochs=epochs,
    validation_data=([X_test[:,:,:,0], X_test[:,:,:,1]], y_test),
    callbacks=[es_callback, modelckpt_callback],
)

plt.plot(np.c_[history.history['accuracy'], history.history['val_accuracy']])
plt.legend(['accuracy', 'val_accuracy'])
plt.show()

plt.plot(np.c_[history.history['loss'], history.history['val_loss']])
plt.legend(['loss', 'val_loss'])
plt.show()

model_predictions = model.predict([X_test[:,:,:,0], X_test[:,:,:,1]])

def convert_to_matrix(logit_list):
    logit_list = list(logit_list)
    return logit_list.index(max(logit_list))

cm = confusion_matrix(
    y_true=list(map(convert_to_matrix, y_test.values)),
    y_pred=list(map(convert_to_matrix, model_predictions)),
)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=selected_modulation_classes)
disp.plot()
plt.show()

model.save_weights('D:/data/Data Signals/CNN_model.weights.h5')

loaded_model = create_model()
loaded_model.load_weights('D:/data/Data Signals/CNN_model.weights.h5')

loss, acc = loaded_model.evaluate([X_test[:,:,:,0], X_test[:,:,:,1]], y_test, verbose=2)
print("Restored model, accuracy: {:5.2f}%".format(100 * acc))
